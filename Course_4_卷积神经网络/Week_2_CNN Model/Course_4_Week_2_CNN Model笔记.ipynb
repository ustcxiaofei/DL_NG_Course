{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**主要内容**\n",
    "- AlexNet LeNet VGG ResNet Inception Network \n",
    "- 1乘1卷积\n",
    "- 迁移学习\n",
    "- 数据扩充"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 几种经典的卷积网络\n",
    "**LeNet-5**\n",
    "- 主要针对灰度图像设计，输入较小\n",
    "- 存在的经典模式：\n",
    "   - 随网络深度增加，图像的大小缩小，通道数量增加\n",
    "   - 每个卷积层后接一个池化层\n",
    "   \n",
    "   \n",
    "![lenet](./Image/lenet.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AlexNet**\n",
    "- 对彩色大图进行处理\n",
    "- 与lenet网络相似，网络结构更大，参数更多，表现也更好\n",
    "- 使用了Relu\n",
    "\n",
    "![alex](./Image/alex.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VGG-16**\n",
    "- 所有卷积层具有相同的卷积核大小 3$*$3,stride = 1 SAME\n",
    "- 所有的池化层都使用f = 2, stride =2\n",
    "\n",
    "![vgg](./Image/vgg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet\n",
    "ResNet由多个残差块构成\n",
    "\n",
    "**残差块**\n",
    "![res](./Image/res.png)\n",
    "\n",
    "**连接在Relu激活函数之前**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Residual Network**\n",
    "![resnet](./Image/resnet.png)\n",
    "\n",
    "- ResNet对于中间的激活函数来说，有助于能够达到更深的网络，解决梯度消失和梯度爆炸的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ResNet表现好的原因**\n",
    "假设有个比较大的神经网络，输入为x，输出为$a^{[l]}$。如果我们想增加网络的深度，这里再给网络增加一个残差块：\n",
    "![res2](./Image/res2.png)\n",
    "\n",
    "假设网络中均使用Relu激活函数，所以最后的输出a⩾0。这里我们给出$a^{[l+2]}$的值：\n",
    "$$a^{[l+2]} = g(z^{[l+2]}+a^{[l]})=g(W^{[l+2]}a^{[l+1]}+b^{[l+2]}+a^{[l]})$$\n",
    "\n",
    "如果使用L2正则化或者权重衰减，会压缩W和b的值，如果$W^{[l+2]}=0$同时$b^{[l+2]}=0$，那么上式就变成：\n",
    "$$a^{[l+2]} = g(z^{[l+2]}+a^{[l]})=g(a^{[l]})=relu(a^{[l]})= a^{[l]}$$\n",
    "\n",
    "所以从上面的结果我们可以看出，对于残差块来学习上面这个恒等函数是很容易的。\n",
    "- 所以在增加了残差块后更深的网络的性能也并不逊色于没有增加残差块简单的网络。所以尽管增加了网络的深度，但是并不会影响网络的性能。\n",
    "- 同时如果增加的网络结构能够学习到一些有用的信息，那么就会提升网络的性能。\n",
    "\n",
    "**注意**\n",
    "- 有两种方式的res block ：identify形式，即经过主路劲后不改变输入大小，conv形式，改变主路大小\n",
    "- shortcut与主路输出add的时候，是直接相加在activation，而不是在通道上进行concat,这从公式上可以看出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1*1卷积\n",
    "\n",
    "**1$*$1卷积**\n",
    "\n",
    "在二维上的卷积相当于图片的每个元素和一个卷积核数字相乘。\n",
    "\n",
    "但是在三维上，与1×1×nC卷积核进行卷积，相当于三维图像上的1×1×nC的切片，也就是nC个点乘以卷积数值权重，通过Relu函数后，输出对应的结果。而不同的卷积核则相当于不同的隐层神经元结点与切片上的点进行一一连接。\n",
    "- 最终三维的图形应用1×11×1的卷积核得到一个相同长宽但第三维度变为卷积核个数的图片。\n",
    "\n",
    "![11](./Image/11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**应用**\n",
    "- 维度压缩：使用目标维度的$1*1$卷积个数\n",
    "- 增加非线性：保持与原维度相同的卷积核个数\n",
    "![112](./Image/112.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inception Network\n",
    "Inception Network 的作用就是使我们无需去考虑在构建深度卷积神经网络时，使用多大的卷积核以及是否添加池化层等问题。\n",
    "\n",
    "**Inception主要结构：**\n",
    "![inception](./Image/inception.png)\n",
    "\n",
    "在上面的Inception结构中，应用了不同的卷积核，以及带padding的池化层。在保持输入图片大小不变的情况下，通过不同运算结果的叠加，增加了通道的数量。\n",
    "\n",
    "**使用$1*1$网络来过渡，降低计算成本**\n",
    "\n",
    "对于1×1大小卷积核用作过渡的计算成本，也将下面的中间的层叫做“bottleneck layer”：\n",
    "\n",
    "![113](./Image/113.png)\n",
    "- 事实证明，只要合理地设置“bottleneck layer”，既可以显著减小上层的规模，同时又能降低计算成本，从而不会影响网络的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inception 模块**\n",
    "将上面2中思路合并，形成一个Inception block\n",
    "\n",
    "![inception2](./Image/inception2.png)\n",
    "\n",
    "**Inception network**\n",
    "![inception3](./Image/inception3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据扩充\n",
    "与其他机器学习问题相比，在计算机视觉领域当下最主要的问题是没有办法得到充足的数据。所以在我们训练计算机数据模型的时候，数据的扩充就是会非常有用。\n",
    "\n",
    "**方法**\n",
    "\n",
    "- 镜像翻转（Mirroring）；\n",
    "- 随机剪裁（Random Cropping）；\n",
    "- 色彩转换（Color shifting）： \n",
    "为图片的RGB三个色彩通道进行增减值，如（R：+20，G：-20，B：+20）；PCA颜色增强：对图片的主色的变化较大，图片的次色变化较小，使总体的颜色保持一致。\n",
    "\n",
    "**tips**\n",
    "\n",
    "为了节约时间，数据扩充与训练过程可以多CPU多线程并行是实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV现状\n",
    "**数据与手工工程**\n",
    "![cv](./Image/cv.png)\n",
    "\n",
    "在有大量数据的时候，我们更倾向于使用简单的算法和更少的手工工程。因为此时有大量的数据，我们不需要为这个问题来精心设计特征，我们使用一个大的网络结果或者更简单的模型就能够解决。\n",
    "\n",
    "相反，在有少量数据的时候，我们从事更多的是手工工程。因为数据量太少，较大的网络结构或者模型很难从这些少量的数据中获取足够的特征，而手工工程实际上是获得良好表现的最佳方式。\n",
    "\n",
    "**Tips for doing well：**\n",
    "\n",
    "在基准研究和比赛中，下面的tips可能会有较好的表现：\n",
    "\n",
    "- Ensembling：独立地训练多个网络模型，输出平均结果或加权平均结果；\n",
    "- 测试时的 Multi-crop：在测试图片的多种版本上运行分类器，输出平均结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
