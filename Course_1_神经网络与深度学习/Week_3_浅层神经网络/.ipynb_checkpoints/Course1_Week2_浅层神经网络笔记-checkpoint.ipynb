{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本部分主要内容包括：\n",
    "* 神经网络\n",
    "* 激活函数\n",
    "* 梯度下降法\n",
    "* 反向传播\n",
    "* 随机初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络的表示\n",
    "![nn represation](./Image/nn represation.png)\n",
    "\n",
    "* 注意其中的参数矩阵规格大小\n",
    "   - $w^{[1]}->(4,3)$:前面的4是后一层神经元的个数，后面的3是前一层输入层神经元的个数\n",
    "   - $b^{[1]}->(4,1)$:4表示后一层输出单元的个数\n",
    "  \n",
    " 由上面我们可以总结出，在神经网络中，我们以相邻两层为观测对象，前面一层作为输入，后面一层作为输出，两层之间的w参数矩阵大小为($n_{out},n_{in}$)，b参数矩阵大小为($n_{out},1$)，这里是作为z=wX+b的线性关系来说明的\n",
    " \n",
    " 在logistic regression中，一般我们都会用($n_{in},n_{out}$)来表示参数大小，计算使用的公式为：$z=w^TX+b$，要注意这两者的区别。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算神经网络的输出\n",
    "![nn_output](./Image/nn_output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 向量化的实现\n",
    "![nn_vec](./Image/nn_vec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 激活函数\n",
    "![activation_function](./Image/activation_function.png)\n",
    "\n",
    "* sigmoid:$a = \\dfrac{1}{1+e^{-z}}$\n",
    "   - 导数：$a' = a(1-a)$\n",
    "* tanh:$a=\\dfrac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$\n",
    "   - 导数：$a'=1-a^{2}$\n",
    "* ReLU:$a = \\max(0,z)$\n",
    "* Leaky ReLu:$a = \\max(0.01z,z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度下降\n",
    "\n",
    "以本节中的浅层神经网络为例，我们给出神经网络的梯度下降法的公式。\n",
    "\n",
    "- 参数：$W^{[1]},b^{[1]},W^{[2]},b^{[2]}$；\n",
    "- 输入层特征向量个数：$n_x=n^{[0]}$；\n",
    "- 隐藏层神经元个数：$n^{[1]}$；\n",
    "- 输出层神经元个数：$n^{[2]}=1$；\n",
    "- $W^{[1]}$的维度为($n^{[1]},n^{[0]}$)，$b^{[1]}$的维度为($n^{[1]},1$)；\n",
    "- $W^{[2]}$的维度为($n^{[2]},n^{[1]}$)，$b^{[2]}$的维度为($n^{[2]},1$)；\n",
    "\n",
    "下面为该例子的神经网络反向梯度下降公式（左）和其代码向量化（右）：\n",
    "![gradient](./Image/gradient.png)\n",
    "\n",
    "**计算的技巧在于根据向量化后的前向传播公式，直接推导出向量化的梯度，注意保证其维度匹配。同时注意，计算dw db的方式都是一致的，只是需要对每层计算dz。而dz的计算每层的方法除了所使用的激活函数不一致外，计算方法也是一样的。为了计算dz，推导时候需要计算中间变量da,而最后实现的时候不需要计算da**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 随机初始化\n",
    "如果在初始时，两个隐藏神经元的参数设置为相同的大小，那么两个隐藏神经元对输出单元的影响也是相同的，通过反向梯度下降去进行计算的时候，会得到同样的梯度大小，所以在经过多次迭代后，两个隐藏层单位仍然是对称的。无论设置多少个隐藏单元，其最终的影响都是相同的，那么多个隐藏神经元就没有了意义。**随机的目的在于打破对称性**\n",
    "\n",
    "在初始化的时候，W参数要进行随机初始化，b则不存在对称性的问题它可以设置为0。\n",
    "\n",
    "以2个输入，2个隐藏神经元为例：\n",
    "```\n",
    "W = np.random.rand((2,2))* 0.01\n",
    "b = np.zero((2,1))\n",
    "```\n",
    "这里我们将W的值乘以0.01是为了尽可能使得权重W初始化为较小的值，这是因为如果使用sigmoid函数或者tanh函数作为激活函数时，W比较小，则Z=WX+b所得的值也比较小，处在0的附近，0点区域的附近梯度较大，能够大大提高算法的更新速度。而如果W设置的太大的话，得到的梯度较小，训练过程因此会变得很慢。\n",
    "\n",
    "ReLU和Leaky ReLU作为激活函数时，不存在这种问题，因为在大于0的时候，梯度均为1。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
