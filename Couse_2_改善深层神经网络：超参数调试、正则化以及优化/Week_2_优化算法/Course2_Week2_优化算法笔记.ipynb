{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**主要内容：**\n",
    "- Mini-batch\n",
    "- 指数加权平均\n",
    "- Momentum 梯度下降\n",
    "- RMSprp\n",
    "- Adam优化算法\n",
    "- 学习力衰减\n",
    "- 局部最优"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-batch\n",
    "* batch梯度下降： \n",
    "   - 对所有m个训练样本执行一次梯度下降，每一次迭代时间较长；\n",
    "   - Cost function 总是向减小的方向下降。\n",
    "* 随机梯度下降： \n",
    "   - 对每一个训练样本执行一次梯度下降，但是丢失了向量化带来的计算加速；\n",
    "   - Cost function总体的趋势向最小值的方向下降，但是无法到达全局最小值点，呈现波动的形式。\n",
    "* Mini-batch梯度下降： \n",
    "   - 选择一个1<size<m1<size<m 的合适的size进行Mini-batch梯度下降，可以实现快速学习，也应用了向量化带来的好处。\n",
    "   - Cost function的下降处于前两者之间。\n",
    "![mini-batch](./Image/mini-batch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 指数加权平均\n",
    "$$v_{t} = \\beta v_{t-1}+(1-\\beta)\\theta_{t}$$\n",
    "![exp](./Image/exp.png)\n",
    "* 当β=0.9时，指数加权平均最后的结果如图中红色线所示；\n",
    "* 当β=0.98时，指数加权平均最后的结果如图中绿色线所示；\n",
    "* 当β=0.5时，指数加权平均最后的结果如下图中黄色线所示；\n",
    "\n",
    "* 偏差修正：$\\dfrac{v_{t}}{1-\\beta^{t}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 动量（Momentum）梯度下降法\n",
    "**计算梯度的指数加权平均数，并利用该梯度来更新权重**\n",
    "\n",
    "- 算法实现\n",
    "![mo1](./Image/mo1.png)\n",
    "- 示意图\n",
    "![mo2](./Image/mo2.png)\n",
    "- 理解\n",
    "在我们进行动量梯度下降算法的时候，由于使用了指数加权平均的方法。原来在纵轴方向上的上下波动，经过平均以后，接近于0，纵轴上的波动变得非常的小；但在横轴方向上，所有的微分都指向横轴方向，因此其平均值仍然很大。最终实现红色线所示的梯度下降曲线。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSprop\n",
    "![rms](./Image/rms.png)\n",
    "利用RMSprop算法，可以减小某些维度梯度更新波动较大的情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam\n",
    "结合Momentum以及RMSprop思想\n",
    "第t次迭代过程如下：\n",
    "![adam](./Image/adam.png)\n",
    "\n",
    "* 超参数的选择\n",
    "   - α需要进行调试；\n",
    "   - β1：常用缺省值为0.9，dw的加权平均；\n",
    "   - β2：推荐使用0.999，$dw^2$的加权平均值；\n",
    "   - ε：推荐使用$10^{-8}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学习率衰减\n",
    "* 常用：$$\\alpha = \\dfrac{1}{1+decay\\_rate*epoch\\_num}\\alpha_{0}$$\n",
    "* 指数衰减：$$\\alpha = 0.95^{epoch\\_num}\\alpha_{0}$$\n",
    "* 其他：$$\\alpha = \\dfrac{k}{epoch\\_num}\\cdot\\alpha_{0}$$\n",
    "* 离散衰减"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 局部最优问题\n",
    "![jubu](./Image/jubu.png)\n",
    "在高维度的情况下： \n",
    "* 几乎不可能陷入局部最小值点； \n",
    "* 处于鞍点的停滞区会减缓学习过程，利用如Adam等算法进行改善。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
