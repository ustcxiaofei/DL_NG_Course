{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**主要内容包括：**\n",
    "- 训练测试集划分\n",
    "\n",
    "- 偏差VS方差\n",
    "\n",
    "- 正则化\n",
    "\n",
    "- DropOut\n",
    "\n",
    "- 输入归一化\n",
    "\n",
    "- 梯度消失及爆炸\n",
    "\n",
    "- 权重初始化\n",
    "\n",
    "- 梯度检验等\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练、验证、测试集\n",
    "**定义**\n",
    "- 训练集：训练模型\n",
    "- 验证集：进行交叉验证选择最佳模型\n",
    "- 测试集：对模型进行性能评估\n",
    "\n",
    "**数据划分**\n",
    "- 小数据时代(100、1000、10000)：无验证集：70/30 有验证集：60/20/20\n",
    "- 大数据时代(百万、超百万)：98/1/1 甚至 99.5/0.25/0.25\n",
    "\n",
    "**注意**\n",
    "- 训练集与验证集要同分布，这样可加快学习\n",
    "- 若无需对算法进行无偏估计，则可以不用测试集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bias vs variance\n",
    "![bv1](./Image/bv1.png)\n",
    "![bv2](./Image/bv2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习的基本方法\n",
    "**在训练机器学习模型的过程中，解决High bias 和High variance 的过程：**\n",
    "![bv3](./Image/bv3.png)\n",
    "- 是否存在High bias ? \n",
    "   - 增加网络结构，如增加隐藏层数目；\n",
    "   - 训练更长时间；\n",
    "   - 寻找合适的网络架构，使用更大的NN结构；\n",
    "- 是否存在High variance？ \n",
    "   - 获取更多的数据；\n",
    "   - 正则化（ regularization）；\n",
    "   - 寻找合适的网络结构；\n",
    "   \n",
    "在大数据时代，深度学习对监督式学习大有裨益，使得我们不用像以前一样太过关注如何平衡偏差和方差的权衡问题，通过以上方法可以使得再不增加另一方的情况下减少一方的值，实现**低偏差且低方差**。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正则化 regularization\n",
    "利用正则化来解决High variance 的问题，正则化是在 **Cost function**中加入一项正则化项，惩罚模型的复杂度。\n",
    "$$J(w,b)=\\dfrac{1}{m}\\sum\\limits_{i=1}^{m}l(\\hat y^{(i)},y^{(i)})+\\dfrac{\\lambda}{2m}||w||_{2}^{2}$$\n",
    "* L2正则化：$$\\dfrac{\\lambda}{2m}||w||_{2}^{2} = \\dfrac{\\lambda}{2m}\\sum\\limits_{j=1}^{n_{x}} w_{j}^{2}=\\dfrac{\\lambda}{2m}w^{T}w$$\n",
    "* L1正则化：$$ \\dfrac{\\lambda}{2m}||w||_{1}=\\dfrac{\\lambda}{2m}\\sum\\limits_{j=1}^{n_{x}}|w_{j}|$$\n",
    "* Frobenius norm: $$||w^{[l]}||_{F}^{2}=\\sum\\limits_{i=1}^{n^{[l-1]}}\\sum\\limits_{j=1}^{n^{[l]}}(w_{ij}^{[l]})^{2}$$\n",
    "其中L2范数正则化也被称为权重衰减"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正则化减小过拟合的解释\n",
    "* 直观上\n",
    "\n",
    "为使代价函数减小，W权重会被设置为近0，从而将复杂的网络变为较小的网络\n",
    "* 数学上\n",
    "![regu](./Image/regu.png)\n",
    "W减小Z减小，从而使得激活函数近似线性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout正则化\n",
    "Dropout（随机失活）就是在神经网络的Dropout层，为每个神经元结点设置一个随机消除的概率，对于保留下来的神经元，我们得到一个节点较少，规模较小的网络进行训练。\n",
    "![dp](./Image/dp.png)\n",
    "* 实现Dropout的方法：反向随机失活（Inverted dropout）\n",
    "```\n",
    "keep_prob = 0.8  # 设置神经元保留概率\n",
    "d3 = np.random.rand(a3.shape[0], a3.shape[1]) < keep_prob\n",
    "a3 = np.multiply(a3, d3)\n",
    "a3 /= keep_prob\n",
    "```\n",
    "   - 最后一步是为了保证下一层的输入期望不变\n",
    "* 在测试阶段不要使用dropout\n",
    "* 另一中理解\n",
    "\n",
    "这里我们以单个神经元入手，单个神经元的工作就是接收输入，并产生一些有意义的输出，但是加入了Dropout以后，输入的特征都是有可能会被随机清除的，所以该神经元不会再特别依赖于任何一个输入特征，也就是说不会给任何一个输入设置太大的权重。\n",
    "\n",
    "所以通过传播过程，dropout将产生和L2范数相同的收缩权重的效果。\n",
    "\n",
    "对于不同的层，设置的keep_prob也不同，一般来说神经元较少的层，会设keep_prob \n",
    "=1.0，神经元多的层，则会将keep_prob设置的较小。\n",
    "* 缺点\n",
    "ropout的一大缺点就是其使得 Cost function不能再被明确的定义，因为每次迭代都会随机消除一些神经元结点，无法画出每次迭代J下降的图\n",
    "* 使用Dropout\n",
    "先关闭dropout，即keep_rob=1.0，确保J单调递减后，在打开"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他正则化方法\n",
    "* 数据增广\n",
    "![da](./Image/da.png)\n",
    "* Early stopping:（无法同时解决bias variance之间的最优）\n",
    "![es](./Image/es.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 归一化输入\n",
    "![nr](./Image/nr.png)\n",
    "* 步骤\n",
    "   - 计算每个特征所有样本数据的均值：$\\mu = \\dfrac{1}{m}\\sum\\limits_{i=1}^{m}x^{(i)}$\n",
    "   - 减去均值获得对称分布：$x:=x-\\mu$\n",
    "   - 归一化方差：$\\sigma^{2} = \\dfrac{1}{m}\\sum\\limits_{i=1}^{m}x^{(i)^{2}} ,   x = x/\\sigma^{2} $\n",
    "   \n",
    "   \n",
    "* 使用归一化的原因\n",
    "![nr2](./Image/nr2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度消失及爆炸\n",
    "![gd](./Image/gd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用初始化缓解梯度消失和爆炸问题\n",
    "当输入的数量n较大时，我们希望每个$w_i$的值都小一些，这样它们的和得到的z也较小。\n",
    "\n",
    "这里为了得到较小的$w_i$，设置$Var(w_i)=\\frac{1}{n}$，这里称为Xavier initialization。\n",
    "```\n",
    "WL = np.random.randn(WL.shape[0],WL.shape[1])* np.sqrt(1/n)\n",
    "```\n",
    "这么做是因为，如果激活函数的输入xx近似设置成均值为0，标准方差1的情况，输出zz也会调整到相似的范围内。虽然没有解决梯度消失和爆炸的问题，但其在一定程度上确实减缓了梯度消失和爆炸的速度。\n",
    "\n",
    "**不同激活函数的 Xavier initialization：**\n",
    "- 激活函数使用Relu：$Var(w_i)=\\frac{2}{n}$\n",
    "- 激活函数使用tanh：$Var(w_i)=\\frac{1}{n}$\n",
    "其中n为输入神经元的个数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度的数值逼近\n",
    "![gd2](./Image/gd2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度的数值检验\n",
    "* **参数连接**\n",
    "![gd3](./Image/gd3.png)\n",
    "* **进行梯度检验**\n",
    "![gd4](./Image/gd3.png)\n",
    "* **判断公式**\n",
    "$$\\dfrac {||d\\theta_{approx}-d\\theta||_{2}}{||d\\theta_{approx}||_{2}+||d\\theta||_{2}}$$\n",
    "* **注意点**\n",
    "   - 不要在训练过程中使用梯度检验，只在debug的时候使用，使用完毕关闭梯度检验的功能；\n",
    "   - 如果算法的梯度检验出现了错误，要检查每一项，找出错误，也就是说要找出哪个dθapprox[i]与dθ的值相差比较大；\n",
    "   - 不要忘记了正则化项；\n",
    "   - 梯度检验不能与dropout同时使用。因为每次迭代的过程中，dropout会随机消除隐层单元的不同神经元，这时是难以计算dropout在梯度下降上的代价函数J；\n",
    "   - 在随机初始化的时候运行梯度检验，\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
